{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing stuff\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aacgmv2, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import math, os, shutil\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import readsav\n",
    "import pymap3d as pm\n",
    "import glob\n",
    "import datetime, statistics\n",
    "from aetherpy.io import read_routines\n",
    "from math import cos, radians, sin, sqrt\n",
    "from scipy import spatial, signal\n",
    "\n",
    "from spacepy.coordinates import Coords\n",
    "from spacepy.time import Ticktock\n",
    "import fnmatch\n",
    "\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from importlib import reload\n",
    "import xarray as xr\n",
    "import geopandas\n",
    "import sys\n",
    "\n",
    "from scipy.interpolate import LinearNDInterpolator, interp1d\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dtime_storm_start = datetime.datetime(2011,5,21,13,40) \n",
    "plot_start_delta  = 4   #hours before storm onset to start making plots\n",
    "plot_end_delta    = 8  # hours after storm onset to end plots. Set to -1 to run for the whole time\n",
    "\n",
    "\n",
    "gitm_cols = ['Rho', '[O(!U3!NP)]', '[O!D2!N]', '[N!D2!N]', '[N(!U4!NS)]', '[NO]', '[He]', '[N(!U2!ND)]', '[N(!U2!NP)]', \n",
    "             '[H]', '[CO!D2!N]', '[O(!U1!ND)]', 'Temperature', 'V!Dn!N(east)', 'V!Dn!N(north)', 'V!Dn!N(up)', 'V!Dn!N(up,O(!U3!NP))',\n",
    "             'V!Dn!N(up,O!D2!N)', 'V!Dn!N(up,N!D2!N)', 'V!Dn!N(up,N(!U4!NS))', 'V!Dn!N(up,NO)', 'V!Dn!N(up,He)', '[O_4SP_!U+!N]', \n",
    "             '[NO!U+!N]', '[O!D2!U+!N]', '[N!D2!U+!N]', '[N!U+!N]', '[O(!U2!ND)!U+!N]', '[O(!U2!NP)!U+!N]', '[H!U+!N]', '[He!U+!N]', \n",
    "             '[e-]', 'eTemperature', 'iTemperature', 'V!Di!N(east)', 'V!Di!N(north)', 'V!Di!N(up)'] # which gitm columns do you want plotted?\n",
    "#TODO: test with not all columns!\n",
    "\n",
    "gitm_path = \"/petastore/phil/GITM/SimStormForSal/GITM-outputs/\"\n",
    "\n",
    "\n",
    "gitm_alt_idxs = -1 #set this to -1 if you want all altitudes\n",
    "gitm_keo_lons = [-90,2,90,-178]\n",
    "\n",
    "global_lat_lim = None # will limit all plots latitude. Must be none or less than keo_lat_lim\n",
    "# ^^ Needs to be tested.\n",
    "\n",
    "keo_lat_lim = 65 # limits keos to +/- degrees of lat. \n",
    "\n",
    "OVERWRITE = True # be careful!\n",
    "\n",
    "num_pool_workers = 40 # number of workers to use in multithreading jobs. Set to 1 if you don't know what this means.\n",
    "\n",
    "both_map_plots = True # make both_map_plots filtered and raw maps? if you only want one put it as a str ('raw'/'filt')\n",
    "\n",
    "sample_rate_min = 5 #min\n",
    "low_cut = 100 # min, lowest freq wave the filter will allow thru\n",
    "high_cut = 30 # min, highest freq the filter will allow thru\n",
    "\n",
    "diff_vs = [1,2,3,5,10,20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gitm_keo_save_path = \"/home/sxe180002/scratch/made_plots/SimStormPaper/keos/\"\n",
    "gitm_map_save_path = \"/home/sxe180002/scratch/made_plots/SimStormPaper/maps/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do GITM Plots First\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gitm_files = np.sort(glob.glob(gitm_path+'3DALL*'))\n",
    "\n",
    "gitm_dtimes = []\n",
    "for i in gitm_files:\n",
    "    yy, MM, dd, hr, mm, sec = i[-17:-15], i[-15:-13], i[-13:-11], i[-10:-8], i[-8:-6], i[-6:-4]\n",
    "    gitm_dtimes.append(datetime.datetime(int('20' + yy), int(MM), int(dd), int(hr), int(mm), int(sec)))\n",
    "    \n",
    "storm_start_index = np.argmin(np.abs(np.array(gitm_dtimes)-dtime_storm_start))\n",
    "plot_start_idx = np.argmin(np.abs(np.array(gitm_dtimes)-(dtime_storm_start - datetime.timedelta(hours = plot_start_delta))))\n",
    "plot_end_idx = np.argmin(np.abs(np.array(gitm_dtimes)-(dtime_storm_start + datetime.timedelta(hours = plot_end_delta)))) if plot_end_delta != -1 else -1\n",
    "\n",
    "# #OVERRIDE varnames\n",
    "# plot_start_idx = storm_start_index - 2\n",
    "# plot_end_idx = storm_start_index + 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in all gitm files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing GITM read routines\n",
    "from utility_programs.read_routines import GITM\n",
    "\n",
    "def make_filter(params = None):\n",
    "    # Define the cutoff frequencies\n",
    "    lowcut = 1/(100/60)  # 100 minutes in units of sample^-1\n",
    "    highcut = 1/(30/60) # 30 minutes in units of sample^-1\n",
    "\n",
    "    # Define the Butterworth filter\n",
    "    nyquist = 0.5 * 5 # 5 minutes is the sampling frequency\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    sos = signal.butter(2, [low, high], btype='bandstop', output='sos')\n",
    "    return sos\n",
    "\n",
    "def make_fits(gitm_bins):\n",
    "    \"\"\"\n",
    "    calculate bandpass filter for all data previously read in.\n",
    "    \n",
    "    inputs: nparray of gitmdata\n",
    "    \n",
    "    returns:\n",
    "    fits: np array indexed at fits[time][col][ilon][ilat][ialt]\n",
    "\n",
    "    \n",
    "    todo: you can thread this by splitting the alts into different threads.\n",
    "    then just append the fits_full later.\n",
    "    \n",
    "    \"\"\"\n",
    "    sos = make_filter()\n",
    "\n",
    "    filtered_arr = signal.sosfiltfilt(sos, gitm_bins, axis=0)\n",
    "    return filtered_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3a387c062346b6b00bcc431b280e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=144.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "times, gitm_grid, gitm_bins, gitmvars = GITM.read_gitm_into_nparrays(gitm_path, dtime_storm_start, t_start_idx=4, t_end_idx=8)\n",
    "lats, lons, alts  = np.unique(gitm_grid['latitude']), np.unique(gitm_grid['longitude']), np.unique(gitm_grid['altitude'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to convert into xarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERSION GOES LIKE THIS:\n",
    "#---------------------------\n",
    "#gitm_bins(array) -> datadict(dict) -> ds(xr.Dataset)\n",
    "\n",
    "# Use this gitmvars when converting to xarray. \n",
    "# xarray doesn't like \"!\" and other special characters\n",
    "xrgitmvars = ['Rho', 'O_3p_', 'O_2', 'N_2', 'N_4s_', 'NO', 'He', 'N_2d_', 'N_2p_', 'H', 'CO_2', 'O_1d_', 'Temperature', 'v_n_east', 'v_n_north', 'v_n_up', 'v_n_up_O_3p_', 'v_n_up_2n_', 'v_n_up_N_2', 'v_n_up_N_4s_', 'v_n_up_NO', 'v_n_up_He', 'O_4_spPlus_', 'NOPlus', 'O_2Plus', 'N_2Plus', 'NPlus', 'O_2dPlus', 'O_2pPlus', 'HPlus', 'HePlus', 'eMinus', 'eTemperature', 'iTemperature', 'v_i_east', 'v_i_north', 'v_i_up']\n",
    "dimnames = ['time', 'lon', 'lat', 'alt']\n",
    "\n",
    "# Loop through data for conversion to dict\n",
    "datadict={}\n",
    "for n, name in enumerate(xrgitmvars):\n",
    "    datadict[name] = (dimnames, gitm_bins[:,n,:,:,:])\n",
    "lats, lons, alts  = np.unique(gitm_grid['latitude']), np.unique(gitm_grid['longitude']), np.unique(gitm_grid['altitude'])\n",
    "\n",
    "# Convert to xarray\n",
    "ds = xr.Dataset(data_vars=datadict,\\\n",
    "                coords=dict(\\\n",
    "                time=times,\\\n",
    "                lon=lons,\\\n",
    "                lat=lats,\\\n",
    "                alt=alts)\n",
    ")\n",
    "del datadict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "NetCDF: HDF error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-54e3cec7310a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mencoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'zlib'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'complevel'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'example.nc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/salhpc/lib/python3.7/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mto_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1910\u001b[0m             \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m             \u001b[0mcompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1912\u001b[0;31m             \u001b[0minvalid_netcdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minvalid_netcdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1913\u001b[0m         )\n\u001b[1;32m   1914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/salhpc/lib/python3.7/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;31m# to be parallelized with dask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         dump_to_store(\n\u001b[0;32m-> 1073\u001b[0;31m             \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u0